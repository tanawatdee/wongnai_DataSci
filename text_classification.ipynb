{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os.path\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from pythainlp import word_vector\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Flatten, Dense, GRU, Bidirectional, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification model\n",
    "- Use _pythainlp_ to tokenize Thai words.\n",
    "- Use _word_vector_ from _pythainlp_ to vectorize Thai words (similar to _Word2vec_ ).\n",
    "- The description texts are converted to sequences in the shape of (dataset_length, max_description_length + 1).\n",
    "- The sequences are then feeded into the Embedding layer and CNN.\n",
    "- As far as I can do the experiments, bidirectional LSTM performs better than CNN, unidirectional LSTM, and bidirectional GRU. Their average accuracies are lower than bidirectional LSTM.\n",
    "- LSTM is suitable for handling input sequence, and bidirectional version help read the context from both forward direction and backward direction.\n",
    "- Adding more Dense layers do not increase accuracy. Only 8-unit Dense layer is enough.\n",
    "- This is a binary classification model; therefore, the ouput node contains the probability that the promotion is spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load a model to vectorize Thai words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = word_vector.get_model() # get a model to vectorize Thai words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the column _description_ of both training & testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text_train = df_train['description'].values # read a column to train the texts\n",
    "x_text_df_test = df_test['description'].values # also include test texts to fit on the same tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenize the description texts. The final result is a list of sentences. Each sentence is a list of words (tokens).\n",
    "- Clean the tokens, eliminate punctuation, extra space, '\\n', and empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lines = list() # 2D list containing word tokens\n",
    "for line in np.concatenate((x_text_train, x_text_df_test)):\n",
    "    tokens = word_tokenize(line)\n",
    "    table  = str.maketrans('', '', string.punctuation + '“”\\n ')\n",
    "    tokens = [token.translate(table) for token in tokens]\n",
    "    tokens = [token for token in tokens if len(token)]\n",
    "    tokens_lines.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the length of training dataset, the maximum sentence length, and the dimension of word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_train_len = len(x_text_train)# length of training dataset\n",
    "token_max_len = int(np.quantile([len(tokens) for tokens in tokens_lines[:token_train_len]], 1)) + 1 # description text\n",
    "embed_dim = len(model_word[model_word.index2word[0]]) # a vector has dimention = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create sequences (list of sentences that each token is substituted by word index number).\n",
    "- Split the training and testing set by the ratio 80:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5002 unique tokens (including an unknown token)\n"
     ]
    }
   ],
   "source": [
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(tokens_lines[:token_train_len]) # create word indexes (number)\n",
    "\n",
    "# create sequences of training texts (substitute each word in tokens_lines by the number)\n",
    "sequences = tokenizer_obj.texts_to_sequences(tokens_lines[:token_train_len])\n",
    "\n",
    "word_index = tokenizer_obj.word_index\n",
    "num_words = len(word_index) + 1\n",
    "print('%s unique tokens (including an unknown token)' % num_words)\n",
    "\n",
    "x_text_train = pad_sequences(sequences, maxlen=token_max_len, padding='pre')\n",
    "\n",
    "# split into train & test\n",
    "x_text_train, x_text_test, y_text_train, y_text_test = train_test_split(x_text_train,\n",
    "                                                                        df_train['bad'],\n",
    "                                                                        test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a matrix of word vectors used to initialize the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((num_words, embed_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    if word in model_word:\n",
    "        embedding_matrix[i] = model_word[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the data is ready to be trained\n",
    "- x is in the shape of (split_ratio*dataset_length, max_description_length + 1)\n",
    "- y is in the shape of (split_ratio*dataset_length,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_text_train\t(1684, 276)\n",
      "y_text_train\t(1684,)\n",
      "x_text_test\t(421, 276)\n",
      "y_text_test\t(421,)\n"
     ]
    }
   ],
   "source": [
    "print('x_text_train\\t(%d, %d)' % x_text_train.shape)\n",
    "print('y_text_train\\t(%d,)' % y_text_train.shape)\n",
    "print('x_text_test\\t(%d, %d)' % x_text_test.shape)\n",
    "print('y_text_test\\t(%d,)' % y_text_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create text classification model consists of embedding layer, bidirectional LSTM, 8-unit Dense layer, and 1-unit output layer.\n",
    "- The embedding layer is pre-trained, given the embedding matrix as an initializer. This layer is not trainable for this model.\n",
    "- The last activation function is sigmoid to output a probability of being spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 276, 300)          1500600   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                85248     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,586,377\n",
      "Trainable params: 85,777\n",
      "Non-trainable params: 1,500,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            embed_dim,\n",
    "                            embeddings_initializer = Constant(embedding_matrix),\n",
    "                            input_length = token_max_len,\n",
    "                            trainable = False)\n",
    "model_text = Sequential()\n",
    "\n",
    "model_text.add(embedding_layer)\n",
    "model_text.add(Bidirectional(LSTM(32)))\n",
    "model_text.add(Dense(8, activation='relu'))\n",
    "model_text.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_text.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the model.\n",
    "- For each epochs, save the model to a file if it performs better.\n",
    "- If it works as expected, the accuracy of validation data should be greater than 0.80.\n",
    "- The training process is running until the accuracy does not improve for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text_file = f'model_text_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_text_file, monitor='val_acc', save_best_only=True, save_weights_only=False, mode='auto', period=1, verbose=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "model_text.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'])\n",
    "\n",
    "model_text.fit(x_text_train, y_text_train,\n",
    "          batch_size=64,\n",
    "          epochs=100,\n",
    "          validation_data=(x_text_test, y_text_test),\n",
    "          shuffle=True,\n",
    "          callbacks=[checkpoint, early])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The testing set splitted before is used to evaluate the model.\n",
    "- The higher recall of the label 1 means the detected real spams are higher among all of real spams (From all of real spam, I can find and detect them).\n",
    "- The higer precision of the label 1 means the detected real spams are higher among all of predicted spams (From all of predicted spam, they are really spams).\n",
    "- Both two values are important for spam detection. The model has to detect the right spams and detect as many spams as possible at the same time.\n",
    "- The F1 score is harmonic mean of those two values. Considering this value is better than either individually too high recall or individually too high precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text = load_model(model_text_file)\n",
    "\n",
    "y_text_pred = model_text.predict(x_text_test)\n",
    "y_text_pred_bool = np.array([y[0] > 0.5 for y in y_text_pred])\n",
    "\n",
    "print(classification_report(y_text_test, y_text_pred_bool))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
